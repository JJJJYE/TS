---
title: "Machine Learning"
format: 
  html:
    theme: cosmo
    smooth-scroll: true
    toc: true
    toc-location: right
    # self-contained: true
# author: 
#     - name: J.I. Seo
#       affiliations:
#       - Gyeongguk National University
#     - name: J.W. Lee
#       # affiliations:
#       # - University of Missouri
      
number-sections: true
highlight-style: pygments
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
options(width=200)
```

## Introduction

> 트리 기반 머신 러닝 기법(Tree-based on Machine Learning Technique)을 시계열에 적용하는 방법에 대해 정리한다.

- 트리 기반 머신 러닝 기법(Tree-based on Machine Learning Technique)은 몇 가지 장점을 가지고 있어 모델링에 널리 사용되는 머신 러닝 방법이다.
    - 이해하기 쉽다.
    - 트리 구조는 시각화 하기에 간단하며, 탐색적 분석에 유용하다.
    - 빠르다.
    - 예측 정확도가 우수하다.
- 하지만, `이상치에 예민하다는 단점`이 있다. 
    - 트리는 Training Data를 바탕으로 간단한 규치만 배우면서 만들어지기 때문이다.
- 그렇기 때문에, `트리 기반 머신러닝 기법은 추세를 포착할 수 없다.`
    - Ref. [1](https://petolau.github.io/Regression-trees-for-forecasting-time-series-in-R/), [2](https://dohk.tistory.com/220), [3](https://medium.com/open-machine-learning-course/open-machine-learning-course-topic-9-time-series-analysis-in-python-a270cb05e0b3), [4](https://stats.stackexchange.com/questions/246853/time-series-and-xgboost), [5](https://stackoverflow.com/questions/45695975/xgboost-time-series-model-does-not-capture-trend)
- 그래서 `분해(Decomposition)`을 이용하여, `추세(Trend)가 제거된 Detrend Data에 트리 기반 머신 러닝 기법을 적용`해야한다.
    - 추세는 적절한 시계열 모형을 이용하여 예측한다.
- `최종 예측 = 머신 러닝의 예측값 + 추세의 예측값 ` 


## Application

- Ridership on Amtrak Trains(미국 철도 회사 “Amtrak”에서 수집한 1991년 1월~2004년 3월까지 매달 환승 고객 수) 예제를 이용하여 트리 기반 머신 러닝 기법이 실제 데이터에 어떻게 적용되는지 설명한다.


### 데이터 불러오기

```{r, eval  = F}
pacman::p_load( "dplyr", "xts", 
                "caret", "caretEnsemble",
                "forecast", 
                "ggplot2")

library(doParallel)
library(parallel)

registerDoParallel(cores=detectCores())

# In Mac
# guess_encoding("Amtrak.csv")
# Amtrak.data <- read.csv("Amtrak.csv", fileEncoding="EUC-KR")

Amtrak.data  <- read.csv(".../Amtrak.csv")

Amtrak.data %>%
  as_tibble
```

```{r, echo=F}
pacman::p_load( "dplyr", "xts", 
                "caret", "caretEnsemble",
                "forecast", 
                "ggplot2")

library(doParallel)
library(parallel)

# cl <- makePSOCKcluster(detectCores())
# clusterEvalQ(cl, library(foreach))
# registerDoParallel(cores=cl)

registerDoParallel(cores=detectCores())

# In Mac
# guess_encoding("Amtrak.csv")
# Amtrak.data <- read.csv("Amtrak.csv", fileEncoding="EUC-KR")

Amtrak.data  <- read.csv("./DATA/Amtrak.csv")

Amtrak.data %>%
  as_tibble
```

### 데이터 전처리

```{r}
# Convert to ts for Target
ridership.ts <- ts(Amtrak.data$Ridership, 
                   start = c(1991, 2),            # 시계열의 시작 연도 / c(1991, 2) : 1991년 2월 -> 첫 번째 시계열은 제거했기 때문
                   frequency = 12)                # 주기 / 월별 시계열로 1년에 12번 관측

# 시계열 그림
plot(ridership.ts, 
     xlab = "Time", ylab = "Ridership (in 000s)",
     ylim = c(1300, 2300))
```

`Caution!` CSV 파일로 불러온 데이터를 살펴보면 승객 수를 포함하는 변수 `Ridership`가 수치형임을 알 수 있다. 시계열 데이터 분석을 위해 함수 `ts()`를 이용하여 해당 변수를 시계열 객체로 변환해야 한다.  
`Result!` 시계열 그림을 살펴보면 `Amtrak` 데이터는 U자 형태의 추세를 발견할 수 있으며, 여름(7월과 8월) 동안에 승객이 급증하는 뚜렷한 계절변동도 볼 수 있다.


### 데이터 분할

`Caution!` 시계열 데이터 분석 시 모형의 과적합을 피하고 미래 데이터에 대한 예측력을 계산하기 위해 `Training Dataset`과 `Test Dataset`으로 분할해야 한다. 시계열 데이터의 경우, 시간에 의존하기 때문에 시간 순서를 고려하여 데이터를 분할해야 한다. 즉, 앞 시점의 데이터를 `Training Dataset`로 사용하여 모형을 구축하고, 뒷 시점의 데이터를 `Test Dataset`로 사용하여 구축된 모형의 성능을 평가한다. 여기서는 데이터 분할을 위해 함수 `window()`를 사용한다.


```{r}
# Partition for Target
train.ts <- window(ridership.ts, 
                   start = c(1991, 2),            # 분할하고자 하는 시계열의 시작 연도 / c(1991, 2) : 1991년 2월 -> 첫 번째 시계열은 제거했기 때문
                   end = c(2001, 3))              # 분할하고자 하는 시계열의 마지막 연도 / c(2001, 3) : 2001년 3월

test.ts <- window(ridership.ts,
                   start = c(2001, 4))            # 분할하고자 하는 시계열의 시작 연도 / c(2001, 4) : 2001년 4월

nTrain   <- length(train.ts)                      # Training Dataset의 데이터 포인트 개수

nTest   <- length(test.ts)                        # Test Dataset의 데이터 포인트 개수

train.ts %>%
  as_tibble

test.ts %>%
  as_tibble

nTrain
nTest
```



### 예측 변수 생성

- 머신 러닝 기법을 적용하기 위해 예측 변수(Predictor Variable)를 생성하였다.
    - 퓨리에 항(Fourier Terms)
    - 달(Month)

```{r}
# 1. Fourier Term
FT      <- fourier(train.ts, K = 2)                  # K : sine, cosine 쌍의 개수/시계열 데이터의 계절 주기가 2개 이상일 때, K는 계절 주기 수만큼 필요
FT.Test <- fourier(train.ts, K = 2, h = nTest)

# 2. Month
xts(ridership.ts, order = as.Date(ridership.ts))
Month  <- as.Date(ridership.ts) %>%                  # Date 추출
  lubridate::month()                                 # Month 추출

## 퓨리에 항과 합치기
Train.X <- cbind("Month"= Month[1:length(train.ts)], FT) 
Test.X  <- cbind("Month"= Month[-(1:length(train.ts))], FT.Test)  
```


### 분해

- 트리 기반 머신러닝 기법은 추세를 포착하기 못하기 떄문에, 추세를 제거한 Detrend Data를 필요로 한다.
- Seasonal and Trend decomposition using Loess (STL) 분해를 이용하여 시계열 데이터를 추세, 계절성과 나머지 성분으로 분해하였다.
    - 만약, 시계열 데이터가 다중 계절성을 갖는다면, `mstl()`을 이용할 수 있다.
- 추세를 제거하고 `계절성과 나머지 성분을 더한 값을 Target으로 한 Training Dataset`을 생성한다.    


```{r}
decomp.ts <- stl(train.ts, s.window = "periodic", robust = TRUE)$time.series 
# decomp.ts <- mstl(Power.msts, s.window = "periodic", robust = TRUE) # 다중 계절성인 경우

# Target without Trend
Target <- decomp.ts %>% 
  data.frame %>%
  rowwise() %>%                                        # 행별로 작업
  dplyr::mutate(y=sum( seasonal, remainder )) %>%      # Target = Season + Remainder => Detrend
  dplyr::select(y)


Train.Data <- cbind(Target, Train.X)
```

### 머신 러닝 적용

- 위에서 생성한 Training Dataset에 머신 러닝 기법을 적용한다.
- `caret` package를 이용하여, 가장 대표적인 `랜덤포레스트(Random Forest)`와 `eXtreme Gradient Boosting(XGBoost)`를 적용해보았다.
    - Hyperparameter는 Random Search를 이용하여 최적의 조합을 찾았다.
- 게다가, `caretEnsemble` package를 이용하여, 앙상블(Ensemble) 기법 `Stacking`도 적용해보았다.
    
#### Random Forest

- Bagging을 이용한 트리 기반 모형이다.
- 나무를 분할할 때 랜덤적으로 후보 예측 변수를 선택함으로써, 생성된 나무들의 연관성은 감소된다.
- Random Forest에서 Hyperparameter는 `mtry`로 트리가 분할될 때 랜덤적으로 선택되는 후보군 예측 변수 갯수이다.


```{r}
set.seed(100)
fitControl <- trainControl(method = "adaptive_cv",   # cv, repeatedcv
                           number = 5,
                           repeats = 5,
                           adaptive = list(min = 5,
                                           alpha = 0.05,
                                           method = "BT",
                                           complete = TRUE),
                           search = "random",
                           allowParallel = TRUE) 

RF <- function(train, tuneLength, ntree = 500, nodesize = 5){
  
  set.seed(100)                                        # seed 고정 For Cross Validation
  caret.rf <- caret::train(y~., data = train, 
                           method = "parRF",           # Tune Parameter : mtry
                           trControl = fitControl,
                           tuneLength = tuneLength,   
                           ntree = ntree,             
                           nodesize = nodesize,        # nodesize : Terminal Node의 최소 크기
                           importance = TRUE)   
  
  return(caret.rf)
  
}

RF.Caret <- RF(Train.Data, 
               tuneLength = 2,     # tuneLength (탐색할 후보 모수 갯수)
               ntree = 100)        # ntree : 생성할 Tree 수

RF.Caret
RF.Caret$finalModel
RF.Caret$finalModel$tuneValue
```


#### XGBoost

- Boosting을 이용한 트리 기반 모형이다.
- 손실함수와 경사하강법을 이용하는 Gradient Boosting의 단점을 해결하기 위해 제안되었다.
- 가장 큰 특징으로는, 병렬 처리로 인해 빠르고 조기 종료가 가능하다는 것이다.
- XGBoost에서 Hyperparameter는 다음과 같다.
   - `nrounds` : 반복 수
   - `max_depth` : Tree의 최대 깊이
   - `eta` : Learning Late
   - `gamma` : 분할하기 위해 필요한 최소 손실 감소, 클수록 분할이 쉽게 일어나지 않음
   - `colsample_bytree` : Tree 생성 때 사용할 예측변수 비율 
   - `min_child_weight` : 한 leaf 노드에 요구되는 관측치에 대한 가중치의 최소 합
   - `subsample` : 모델 구축시 사용할 Data비율로 1이면 전체 Data 사용

```{r}
set.seed(100)
fitControl <- trainControl(method = "adaptive_cv",   # cv, repeatedcv
                           number = 5,
                           repeats = 5,
                           adaptive = list(min = 5,
                                           alpha = 0.05,
                                           method = "BT",
                                           complete = TRUE),
                           search = "random",
                           allowParallel = TRUE) 

XGBoost <- function(train, tuneLength){
  
  set.seed(100)                                         # seed 고정 For Cross Validation
  caret.xgb <- caret::train(y~., data = train, 
                            method = "xgbTree",          
                            trControl = fitControl,
                            # objective = "reg:squarederror", # error(The following parameters were provided multiple times)
                            tuneLength = tuneLength     # tuneLength (탐색할 후보 모수 갯수)
  )   
  
  return(caret.xgb)
  
}

XGB.Caret <- XGBoost(Train.Data, 2)
XGB.Caret
XGB.Caret$finalModel
XGB.Caret$finalModel$tuneValue
```


#### Stacking

- Stacking은 앙상블 기법 중 하나로, 비슷한 성능을 가진 모형들을 결합함으로써, 예측 성능을 항상 시키기 위해 적용될 수 있다.
- Stacking은 두 종류의 모형들이 필요하다.
    - Individual Model : 원본 Training Data을 이용하여 학습할 모형
        - Individual Model은 `여러개의 서로 다른 머신 러닝 알고리즘로 구성`
    - Final Model : Individual Model들의 예측 결과를 결합한 Data를 Training Data로 사용하여 학습할 모형
- Stacking의 적용 알고리즘은 다음과 같다.
    - 원본 Training Data에 `서로 다른 머신 러닝 알고리즘`을 이용하여 예측 결과를 생성한다.
    - 생성된 예측 결과를 결합하여 Final Model에 대한 Training Data를 생성한다.
    - 위에서 생성된 Training Data를 이용하여 Final Model을 학습시킨다.
    - 원본 Training Data를 기반으로 학습한 모형에 원본 Test Data를 이용하여 예측 결과를 생성한다.
    - 생성된 예측 결과를 결합하여 Final Model에 대한 Test Data를 생성한다.
    - 위에서 생성된 Test Data를 이용하여 Final Model의 예측 결과와 원본 Test Data의 Target과 비교한다.
- 자세한 코드는 [여기](https://github.com/zachmayer/caretEnsemble/blob/master/R/caretStack.R)를 참조한다.    
- 앞써, 원본 Training Data에 적합시킨 Random Forest와 XGBoost의 Hyperparameter 결과를 기반으로 생성된 예측 결과들을 결합하여 스태킹하였다.   
- `caretEnsemble` package의 `caretList()`를 이용하여, Individual Model을 선언하고, `caretStack()`으로 Final Model을 선언한다.
    
    
```{r}
# Ref. https://cran.r-project.org/web/packages/caretEnsemble/vignettes/caretEnsemble-intro.html
#      https://github.com/zachmayer/caretEnsemble/blob/master/R/caretStack.R


# 1. Modeling for Stacking (Declare Individual Model)
set.seed(100)
fitControl <- trainControl(method = "repeatedcv",        # adaptive_cv
                           number = 5,
                           repeats = 5,
                           # adaptive = list(min = 5,
                           #                 alpha = 0.05,
                           #                 method = "BT",
                           #                 complete = TRUE),
                           # search = "random",            # grid
                           savePredictions = "final",      # 최적 모수에 대한 예측 저장
                           # classProbs = TRUE,            # 각 클래스에 대한 확률 저장(Classification)
                           index = createResample(Train.Data$y, 1),  # index : 각 resapling에 대한 요소, Training에 사용되는 행 번호/ createResample : 붓스트랩
                           allowParallel = TRUE) 


# 원본 Training Data에 학습시킨 Hyperparameter 결과
alg_tune_list <- list(                            # Do not use custom names in list. Will give prediction error with greedy ensemble. Bug in caret.
  parRF = caretModelSpec(method="parRF",
                         importance = TRUE,
                         nodeside = 5,
                         tuneGrid = expand.grid(mtry=RF.Caret$finalModel$tuneValue$mtry)),
  xgbTree = caretModelSpec(method="xgbTree",
                           tuneGrid = expand.grid(nrounds = XGB.Caret$finalModel$tuneValue$nrounds,
                                                  max_depth = XGB.Caret$finalModel$tuneValue$max_depth,
                                                  eta = XGB.Caret$finalModel$tuneValue$eta,
                                                  gamma = XGB.Caret$finalModel$tuneValue$gamma,
                                                  colsample_bytree = XGB.Caret$finalModel$tuneValue$colsample_bytree,
                                                  min_child_weight = XGB.Caret$finalModel$tuneValue$min_child_weight,
                                                  subsample = XGB.Caret$finalModel$tuneValue$subsample)))

set.seed(100)
multi_mod <- caretList(y~., data = Train.Data, trControl = fitControl, 
                       tuneList = alg_tune_list)  # search = "grid"     


multi_mod$parRF
multi_mod$xgbTree
```
    

```{r}
# 2. Stacking (개별 모형들의 예측값을 결합한 Data를 Training data로 쓰는 Final Model)

set.seed(100)
stackControl <- trainControl(method = "adaptive_cv",
                             number = 5,
                             repeats = 5,
                             adaptive = list(min = 5,
                                             alpha = 0.05,
                                             method = "BT",
                                             complete = TRUE),
                             search = "random",
                             allowParallel = TRUE) 

set.seed(100)
stack.xgb <- caretStack(multi_mod, method = "xgbTree",  # Final Model
                        trControl = stackControl, 
                        tuneLength = 2)                 # 모수 후보 갯수


stack.xgb
stack.xgb$ens_model$finalModel
stack.xgb$ens_model$finalModel$tuneValue
```
  

#### Stacking with GLM

- Stacking의 Final Model를 Generalized Linear Model(GLM)으로 하면, Individual Model들의 예측 결과들에 가중치를 곱하여 더한 것과 같다. 
- `caretEnsemble` package의 `caretEnsemble()`를 이용하면 Final Model이 `GLM`이며, [여기](https://github.com/zachmayer/caretEnsemble/blob/master/R/caretEnsemble.R)의 R code를 확인하면 된다.
- 최종 예측 결과 $\hat{Y}$는 다음과 같다.

$$
\begin{aligned}
	       \hat{Y} = \beta_{0} + \sum^{m}_{i=1}  \hat{y}_{i}
\end{aligned}
$$

- $m$ : Individual Model 개수
- $\hat{y}_{i}$ : $i$번째 Individual Model의 예측 결과


```{r}
set.seed(100)
greedyEnsemble <- caretEnsemble(multi_mod, trControl = trainControl(method = "cv", number=5))
greedyEnsemble
greedyEnsemble$ens_model$finalModel
summary(greedyEnsemble)
```



##### 변수 중요도

```{r}
VI <- varImp(greedyEnsemble) 
varImp(greedyEnsemble)

VI$x <- as.character(row.names(VI))
VI2 <- reshape2::melt(VI, id.vars="x",
                      variable.name="Type",
                      value.name='VI')

ggplot(VI2, aes(x=reorder(x, VI), y=VI)) + 
  geom_bar(stat="identity", position=position_dodge(), show.legend = F) + 
  facet_wrap(.~ Type, nrow=1) +
  xlab("") +      
  ylab("") +
  coord_flip() +
  theme_bw() +
  theme(axis.text.x = element_text(size = 13),  # angle = 45, vjust = 1, hjust = 1
        axis.text.y = element_text(size = 13),
        axis.title = element_blank(),
        plot.title = element_blank(),
        legend.title = element_blank(),
        strip.text.x = element_text(size=15, color="black"))

```


### 예측

- 최종 예측은 추세의 예측값과 머신 러닝 기법에 의한 예측값을 더하여 생성된다.
    - 추세는 시계열 모형 (예: ARIMA)을 이용하여 예측

#### 추세 예측

- 여기에서는 ARIMA 모형을 이용하여 추세를 예측하였다.

```{r}
# Trend 
trend.part        <- data.frame(decomp.ts)$trend %>% # Only Trend of Training Data
  ts()

# Fitting ARIMA for Trend 
trend.fit.arima   <- auto.arima(trend.part)

# Forecast 
trend.arima.pred  <- forecast(trend.fit.arima, nTest)
trend.arima.pred$mean 

```

#### 최종 예측


```{r}
# 최종 예측(Seasonal + Remainder + Trend)
Pred.RF        <- predict(RF.Caret, Test.X) + trend.arima.pred$mean   
Pred.XGB       <- predict(XGB.Caret, Test.X) + trend.arima.pred$mean
stack.Pred.XGB <- predict(stack.xgb, Test.X) + trend.arima.pred$mean
stack.Pred.GLM <- predict(greedyEnsemble, Test.X) + trend.arima.pred$mean
```



```{r}
# Accuracy
acc_RF        <- accuracy(c(Pred.RF), test.ts)
acc_XGB       <- accuracy(c(Pred.XGB), test.ts)
acc_stack.XGB <- accuracy(c(stack.Pred.XGB), test.ts)
acc_stack.GLM <- accuracy(c(stack.Pred.GLM), test.ts)

acc_RF
acc_XGB
acc_stack.XGB
acc_stack.GLM
```

